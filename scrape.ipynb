{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mongoPass'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee9174be1f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the mongodb password from an environment variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmongoPass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mongoPass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Establish the remote connection to the mongo data base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmyclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMongoClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongodb+srv://axme100:{}@cluster0-5jopz.mongodb.net/test?retryWrites=true&w=majority\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmongoPass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mongoPass'"
     ]
    }
   ],
   "source": [
    "# Get the mongodb password from an environment variable\n",
    "mongoPass = os.environ['mongoPass']\n",
    "\n",
    "# Establish the remote connection to the mongo data base\n",
    "myclient = pymongo.MongoClient(\"mongodb+srv://axme100:{}@cluster0-5jopz.mongodb.net/test?retryWrites=true&w=majority\".format(mongoPass))\n",
    "\n",
    "# This is the name of the cluster stored on mongo atlas\n",
    "mydb = myclient[\"finalProject\"]\n",
    "\n",
    "# Create a new colection called raw article \n",
    "mycol = mydb[\"rawArticles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class that will be created\n",
    "class rawArticle:\n",
    "    def __init__(self, url, title, date, publication, articleText):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.date = date\n",
    "        self.publication = publication\n",
    "        self.articleText = articleText\n",
    "        \n",
    "    def saveToDatabase(self):\n",
    "        # Save the entry into the mongo database\n",
    "        mycol.insert_one({'url': self.url,\n",
    "                         'title': self.title,\n",
    "                         'date': self.date,\n",
    "                         'publication': self.publication,\n",
    "                         'articleText': self.articleText})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class of scraper\n",
    "class scraper:\n",
    "    \n",
    "    def scrape_jornada(self):\n",
    "        # Hard coded URL to be scraped\n",
    "        url=\"https://www.jornada.com.mx/rss/edicion.xml?v=1\"\n",
    "        \n",
    "        # Parse the XML feed\n",
    "        feedBurner = feedparser.parse(url)\n",
    "        \n",
    "        # Loop through all of the entries\n",
    "        for entry in feedBurner['entries']:\n",
    "    \n",
    "            # Get the page that we want to scrape\n",
    "            html = requests.get(entry['link'])\n",
    "        \n",
    "            # Save the page as a beautiful soup object\n",
    "            bs = BeautifulSoup(html.text, 'html.parser')\n",
    "    \n",
    "            articleText = bs.find(\"div\", {\"id\": \"article-text\"}).get_text().replace('\\n','').strip()\n",
    "    \n",
    "            # Create an article object\n",
    "            myArticle = rawArticle(url=entry['link'],\n",
    "                          title=entry['title'],\n",
    "                          date=entry['published'],\n",
    "                          publication=\"La Jornada\",\n",
    "                          articleText=articleText)\n",
    "        \n",
    "            # Call the object's method to save to a mongo data base\n",
    "            myArticle.saveToDatabase()\n",
    "    \n",
    "    def scrape_la_presna():\n",
    "        \n",
    "        # Get the soup of the page where the sub rss links are posted\n",
    "        url='http://laprensa.mx/rss.asp'\n",
    "        html = requests.get(url)\n",
    "        bs = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        # Get a list of all the links that are part of the sub rss feed\n",
    "        rssSections = bs.find_all(\"div\", {\"class\": \"secc\"})\n",
    "        rssSubLinks = []\n",
    "        for section in rssSections:\n",
    "            link = section.find('a', href=True)\n",
    "            rssSubLinks.append(link['href'])\n",
    "        \n",
    "        \n",
    "        # Iterate over and parse each of the links in the rssSubLinks\n",
    "        for link in rssSubLinks:\n",
    "            feedBurner = feedparser.parse(link)\n",
    "    \n",
    "            # Iterate over and parse each of the entries in the rss page\n",
    "            for entry in feedBurner['entries']:\n",
    "                \n",
    "                # Get the soup page of to scrape\n",
    "                html = requests.get(entry['link'])\n",
    "                bs = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # It appears that all of the text is located in a div caled text\n",
    "                textDiv = bs.find('div', {'id': 'texto'})\n",
    "\n",
    "                # Get rid of all the divs that are within this div because\n",
    "                # some pages have tables and images with text that appear to be wrapped in divs\n",
    "                for div in textDiv(\"div\"):\n",
    "                    div.decompose()\n",
    "    \n",
    "                # The result of the previous blocks, \n",
    "                articleText = textDiv.get_text().replace('\\n','').strip()\n",
    "                \n",
    "                # Create an article object\n",
    "                myArticle = rawArticle(url=entry['link'],\n",
    "                          title=entry['title'],\n",
    "                          date=entry['published'],\n",
    "                          publication=\"La Jornada\",\n",
    "                          articleText=articleText)\n",
    "        \n",
    "                # Call the object's method to save to a mongo data base\n",
    "                myArticle.saveToDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af34db573343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyScrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyScrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_jornada\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-a9c6191a790c>\u001b[0m in \u001b[0;36mscrape_jornada\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0marticleText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"article-text\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Create an article object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "myScrapper = scraper()\n",
    "myScrapper.scrape_jornada()\n",
    "myScrapper.scarpe_la_prensa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
